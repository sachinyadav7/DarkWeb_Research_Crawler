import urllib.robotparser
from urllib.parse import urlparse

def allowed_by_robots(url, user_agent='Research-Crawler/1.0'):
    parsed = urlparse(url)
    robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
    rp = urllib.robotparser.RobotFileParser()
    rp.set_url(robots_url)
    try:
        rp.read()
        return rp.can_fetch(user_agent, url)
    except Exception:
        # if robots can't be read, be conservative and return False or True as policy demands
        return False
